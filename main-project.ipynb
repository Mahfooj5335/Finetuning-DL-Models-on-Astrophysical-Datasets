{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3175,"databundleVersionId":44352,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Submitted By Mahfooj Ali\n## Finetuning Deep Learning Models on Astrophysical Datasets","metadata":{}},{"cell_type":"markdown","source":"## Import necessary libraries\n","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport zipfile\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nfrom matplotlib.image import imread\nfrom PIL import Image\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom transformers import ResNetForImageClassification, AutoProcessor\nfrom transformers.optimization import get_scheduler\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:40:53.154997Z","iopub.execute_input":"2025-07-02T14:40:53.155730Z","iopub.status.idle":"2025-07-02T14:41:22.423284Z","shell.execute_reply.started":"2025-07-02T14:40:53.155703Z","shell.execute_reply":"2025-07-02T14:41:22.422575Z"}},"outputs":[{"name":"stderr","text":"2025-07-02 14:41:09.253811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751467269.472240      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751467269.539163      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Unzipping and Data Loading\n","metadata":{}},{"cell_type":"code","source":"# unzipping data and data laoding \n\ndef unzip(file, destination):\n    print('Unzipping to', destination)\n    with zipfile.ZipFile(file, 'r') as zip_ref:\n        zip_ref.extractall(destination)\n\nbase_dir = \"/kaggle/tmp/\"\ntrain_images_path = os.path.join(base_dir, \"images_training_rev1\")\ntest_images_path = os.path.join(base_dir, \"images_test_rev1\")\n\nif not os.path.exists(base_dir):\n    unzip('/kaggle/input/galaxy-zoo-the-galaxy-challenge/images_training_rev1.zip', base_dir)\n    unzip('/kaggle/input/galaxy-zoo-the-galaxy-challenge/images_test_rev1.zip', base_dir)\n    unzip('/kaggle/input/galaxy-zoo-the-galaxy-challenge/training_solutions_rev1.zip', base_dir)\n\nprint(\"Files extracted successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:41:22.424375Z","iopub.execute_input":"2025-07-02T14:41:22.424944Z","iopub.status.idle":"2025-07-02T14:42:14.269606Z","shell.execute_reply.started":"2025-07-02T14:41:22.424923Z","shell.execute_reply":"2025-07-02T14:42:14.268587Z"}},"outputs":[{"name":"stdout","text":"Unzipping to /kaggle/tmp/\nUnzipping to /kaggle/tmp/\nUnzipping to /kaggle/tmp/\nFiles extracted successfully!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Load training solutions\n","metadata":{}},{"cell_type":"code","source":"train_sol = pd.read_csv(\"/kaggle/tmp/training_solutions_rev1.csv\")\ntrain_sol[\"GalaxyID\"] = train_sol[\"GalaxyID\"].apply(lambda x: f\"{x}.jpg\")\nprint('Training data shape', {train_sol.shape})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:42:18.616474Z","iopub.execute_input":"2025-07-02T14:42:18.616994Z","iopub.status.idle":"2025-07-02T14:42:18.940572Z","shell.execute_reply.started":"2025-07-02T14:42:18.616966Z","shell.execute_reply":"2025-07-02T14:42:18.939832Z"}},"outputs":[{"name":"stdout","text":"Training data shape {(61578, 38)}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Enhanced Data Preprocessing\n","metadata":{}},{"cell_type":"code","source":"# Instead of just using the max class, let's use a weighted approach for better class balance\ntrain_sol_features = train_sol.drop('GalaxyID', axis=1)\n\n# Create multi-label approach - use top 3 classes instead of just max\ncategorical_labels = []\nconfidence_scores = []\n\nfor i in range(len(train_sol_features)):\n    row_values = train_sol_features.iloc[i].values\n    sorted_indices = np.argsort(row_values)[::-1]\n    top_3_indices = sorted_indices[:3]\n    top_3_values = row_values[top_3_indices]\n    \n    # Use weighted voting for more robust classification\n    if top_3_values[0] > 0.6:  # High confidence in primary class\n        main_class = train_sol_features.columns[top_3_indices[0]]\n    elif top_3_values[0] > 0.4 and top_3_values[1] > 0.2:  # Mixed features\n        # Choose based on the most discriminative features\n        main_class = train_sol_features.columns[top_3_indices[0]]\n    else:\n        main_class = train_sol_features.columns[top_3_indices[0]]\n    \n    categorical_labels.append(main_class)\n    confidence_scores.append(top_3_values[0])\n\n# Convert to DataFrame\nenhanced_df = pd.DataFrame({\n    'GalaxyID': train_sol['GalaxyID'],\n    'categorical_max': categorical_labels,\n    'confidence': confidence_scores\n})\n\nprint(\"Class distribution:\")\nprint(enhanced_df['categorical_max'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:42:21.633075Z","iopub.execute_input":"2025-07-02T14:42:21.633760Z","iopub.status.idle":"2025-07-02T14:42:23.422003Z","shell.execute_reply.started":"2025-07-02T14:42:21.633732Z","shell.execute_reply":"2025-07-02T14:42:23.421177Z"}},"outputs":[{"name":"stdout","text":"Class distribution:\ncategorical_max\nClass6.2     36707\nClass1.2     11243\nClass1.1      5407\nClass2.2      4398\nClass6.1      3425\nClass11.2      141\nClass3.2       101\nClass8.1        44\nClass4.1        41\nClass7.2        40\nClass1.3        13\nClass9.1         8\nClass4.2         3\nClass9.3         2\nClass7.3         2\nClass11.3        1\nClass8.3         1\nClass10.3        1\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Filter to top 3 classes for better balance and higher accuracy\ntop_three_classes = enhanced_df['categorical_max'].value_counts().index[:3]\nfiltered_df = enhanced_df[enhanced_df['categorical_max'].isin(top_three_classes)].reset_index(drop=True)\n\nprint(\"nFiltered dataset shape:\", {filtered_df.shape})\nprint(\"Filtered class distribution:\")\nprint(filtered_df['categorical_max'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:42:28.211992Z","iopub.execute_input":"2025-07-02T14:42:28.212616Z","iopub.status.idle":"2025-07-02T14:42:28.232452Z","shell.execute_reply.started":"2025-07-02T14:42:28.212592Z","shell.execute_reply":"2025-07-02T14:42:28.231622Z"}},"outputs":[{"name":"stdout","text":"nFiltered dataset shape: {(53357, 3)}\nFiltered class distribution:\ncategorical_max\nClass6.2    36707\nClass1.2    11243\nClass1.1     5407\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Enhanced Label Mapping\n","metadata":{}},{"cell_type":"code","source":"class_to_idx = {class_name: idx for idx, class_name in enumerate(top_three_classes)}\nidx_to_class = {idx: class_name for class_name, idx in class_to_idx.items()}\nfiltered_df['label'] = filtered_df['categorical_max'].map(class_to_idx)\n\nprint(f\"\\nClass mapping: {class_to_idx}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:42:31.215668Z","iopub.execute_input":"2025-07-02T14:42:31.216470Z","iopub.status.idle":"2025-07-02T14:42:31.225991Z","shell.execute_reply.started":"2025-07-02T14:42:31.216445Z","shell.execute_reply":"2025-07-02T14:42:31.225192Z"}},"outputs":[{"name":"stdout","text":"\nClass mapping: {'Class6.2': 0, 'Class1.2': 1, 'Class1.1': 2}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Data Augmentation with stronger transforms\n","metadata":{}},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.3),\n    transforms.RandomRotation(30),\n    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n    transforms.RandomGrayscale(p=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    transforms.RandomErasing(p=0.1, scale=(0.02, 0.33))\n])\n\nvalid_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:42:34.741848Z","iopub.execute_input":"2025-07-02T14:42:34.742463Z","iopub.status.idle":"2025-07-02T14:42:34.749319Z","shell.execute_reply.started":"2025-07-02T14:42:34.742435Z","shell.execute_reply":"2025-07-02T14:42:34.748401Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Enhanced Dataset Class\n","metadata":{}},{"cell_type":"code","source":"class GalaxyDataset(Dataset):\n    def __init__(self, dataframe, images_dir, transform=None):\n        self.dataframe = dataframe\n        self.images_dir = images_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.images_dir, self.dataframe.iloc[idx][\"GalaxyID\"])\n        \n        try:\n            image = Image.open(img_name).convert(\"RGB\")\n        except:\n            # Create a black image if file is corrupted\n            image = Image.new('RGB', (224, 224), (0, 0, 0))\n        \n        if self.transform:\n            image = self.transform(image)\n\n        if \"label\" in self.dataframe.columns:\n            label = self.dataframe.iloc[idx][\"label\"]\n            return image, label\n        else:\n            return image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:42:37.463827Z","iopub.execute_input":"2025-07-02T14:42:37.464377Z","iopub.status.idle":"2025-07-02T14:42:37.470408Z","shell.execute_reply.started":"2025-07-02T14:42:37.464353Z","shell.execute_reply":"2025-07-02T14:42:37.469636Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Stratified train-test split for better balance\n","metadata":{}},{"cell_type":"code","source":"train_val_df, test_df = train_test_split(\n    filtered_df, \n    test_size=0.15, \n    random_state=42, \n    stratify=filtered_df[\"label\"]\n)\n\ntrain_df, valid_df = train_test_split(\n    train_val_df, \n    test_size=0.15, \n    random_state=42, \n    stratify=train_val_df[\"label\"]\n)\n\nprint(f\"\\nDataset splits:\")\nprint(f\"Train: {len(train_df)} samples\")\nprint(f\"Validation: {len(valid_df)} samples\") \nprint(f\"Test: {len(test_df)} samples\")\n\n# Create datasets and dataloaders\ntrain_dataset = GalaxyDataset(train_df, train_images_path, transform=train_transform)\nvalid_dataset = GalaxyDataset(valid_df, train_images_path, transform=valid_transform)\ntest_dataset = GalaxyDataset(test_df, train_images_path, transform=test_transform)\n\n# Use larger batch size for better gradient estimates\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n\nprint(f\"Training batches: {len(train_loader)}\")\nprint(f\"Validation batches: {len(valid_loader)}\")\nprint(f\"Test batches: {len(test_loader)}\")\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:42:41.806520Z","iopub.execute_input":"2025-07-02T14:42:41.806783Z","iopub.status.idle":"2025-07-02T14:42:41.859931Z","shell.execute_reply.started":"2025-07-02T14:42:41.806767Z","shell.execute_reply":"2025-07-02T14:42:41.859253Z"}},"outputs":[{"name":"stdout","text":"\nDataset splits:\nTrain: 38550 samples\nValidation: 6803 samples\nTest: 8004 samples\nTraining batches: 603\nValidation batches: 107\nTest batches: 126\nUsing device: cuda\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"image_tensor,label= test_dataset[8000]\nlabel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:43:13.223044Z","iopub.execute_input":"2025-07-02T14:43:13.223417Z","iopub.status.idle":"2025-07-02T14:43:13.235517Z","shell.execute_reply.started":"2025-07-02T14:43:13.223391Z","shell.execute_reply":"2025-07-02T14:43:13.234651Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"## Evaluating pretrained modal (resNet-50) without finetunning on Test split ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import ResNetForImageClassification\nfrom sklearn.metrics import accuracy_score\nfrom torch import nn, optim\nfrom tqdm import tqdm  # Progress bar\nfrom transformers.optimization import get_scheduler\n\n# Load the pre-trained ResNet-50 model from Hugging Face\nmodel = ResNetForImageClassification.from_pretrained(\n    \"microsoft/resnet-50\", \n    num_labels=2, \n    ignore_mismatched_sizes=True\n)\nmodel = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Placeholder for predictions and ground-truth labels\nall_preds = []\nall_labels = []\n# Disable gradient computation for evaluation\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(model.device), labels.to(model.device)\n        \n        # Forward pass\n        outputs = model(images).logits\n        \n        # Predicted class (highest logit)\n        preds = torch.argmax(outputs, dim=1)\n        \n        # Append predictions and labels\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Calculate test accuracy\naccuracy = accuracy_score(all_labels, all_preds)\nprint(f\"Test Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:08:26.605399Z","iopub.execute_input":"2025-07-02T17:08:26.605739Z","iopub.status.idle":"2025-07-02T17:08:51.760739Z","shell.execute_reply.started":"2025-07-02T17:08:26.605710Z","shell.execute_reply":"2025-07-02T17:08:51.759677Z"}},"outputs":[{"name":"stderr","text":"Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([2, 2048]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 0.6784\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"## finetunning the pre-trained model(ResNet-50) on the train split of galaxyzoo dataset","metadata":{}},{"cell_type":"code","source":"\n# Load ResNet-50 for fine-tuning\nmodel_finetuned = ResNetForImageClassification.from_pretrained(\n    \"microsoft/resnet-50\", \n    num_labels=len(top_three_classes), \n    ignore_mismatched_sizes=True\n)\nmodel_finetuned = model_finetuned.to(device)\n\n# Enhanced optimizer with different learning rates for different parts\nbackbone_params = []\nclassifier_params = []\n\nfor name, param in model_finetuned.named_parameters():\n    if 'classifier' in name:\n        classifier_params.append(param)\n    else:\n        backbone_params.append(param)\n\n# Use different learning rates for backbone and classifier\noptimizer = optim.AdamW([\n    {'params': backbone_params, 'lr': 1e-5},  # Lower LR for pre-trained features\n    {'params': classifier_params, 'lr': 1e-3}  # Higher LR for new classifier\n], weight_decay=0.01)\n\n# Enhanced scheduler\nnum_epochs = 10\nnum_training_steps = len(train_loader) * num_epochs\nwarmup_steps = num_training_steps // 10\n\nscheduler = get_scheduler(\n    \"cosine\", \n    optimizer=optimizer, \n    num_warmup_steps=warmup_steps, \n    num_training_steps=num_training_steps\n)\n\n# Enhanced loss function with label smoothing\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n# Early stopping parameters\nbest_val_accuracy = 0\npatience = 3\npatience_counter = 0\n\nprint(\"Starting standard fine-tuning...\")\n\nfor epoch in range(num_epochs):\n    # Training phase\n    model_finetuned.train()\n    train_loss = 0\n    train_correct = 0\n    train_total = 0\n    \n    for batch_idx, (images, labels) in enumerate(train_loader):\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model_finetuned(images).logits\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        # Gradient clipping for stability\n        torch.nn.utils.clip_grad_norm_(model_finetuned.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        scheduler.step()\n        \n        train_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        train_total += labels.size(0)\n        train_correct += (predicted == labels).sum().item()\n        \n        if batch_idx % 100 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n    \n    train_accuracy = train_correct / train_total\n    avg_train_loss = train_loss / len(train_loader)\n    \n    # Validation phase\n    model_finetuned.eval()\n    val_correct = 0\n    val_total = 0\n    val_loss = 0\n    \n    with torch.no_grad():\n        for images, labels in valid_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model_finetuned(images).logits\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            \n            _, predicted = torch.max(outputs.data, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n    \n    val_accuracy = val_correct / val_total\n    avg_val_loss = val_loss / len(valid_loader)\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}]:')\n    print(f'  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}')\n    print(f'  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n    \n    # Early stopping and model saving\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        torch.save(model_finetuned.state_dict(), \"best_finetuned_model.pth\")\n        patience_counter = 0\n        print(f\"  New best model saved! Val Acc: {val_accuracy:.4f}\")\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"  Early stopping triggered after {epoch+1} epochs\")\n            break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:45:44.620773Z","iopub.execute_input":"2025-07-02T14:45:44.621612Z","iopub.status.idle":"2025-07-02T15:53:17.939327Z","shell.execute_reply.started":"2025-07-02T14:45:44.621577Z","shell.execute_reply":"2025-07-02T15:53:17.938455Z"}},"outputs":[{"name":"stderr","text":"Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([3, 2048]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Starting standard fine-tuning...\nEpoch [1/10], Batch [0/603], Loss: 1.0933\nEpoch [1/10], Batch [100/603], Loss: 0.8623\nEpoch [1/10], Batch [200/603], Loss: 0.8561\nEpoch [1/10], Batch [300/603], Loss: 0.8507\nEpoch [1/10], Batch [400/603], Loss: 0.8719\nEpoch [1/10], Batch [500/603], Loss: 0.8143\nEpoch [1/10], Batch [600/603], Loss: 0.7562\nEpoch [1/10]:\n  Train Loss: 0.8685, Train Acc: 0.6838\n  Val Loss: 0.8244, Val Acc: 0.7050\n  New best model saved! Val Acc: 0.7050\nEpoch [2/10], Batch [0/603], Loss: 0.7361\nEpoch [2/10], Batch [100/603], Loss: 0.7377\nEpoch [2/10], Batch [200/603], Loss: 0.7498\nEpoch [2/10], Batch [300/603], Loss: 0.8047\nEpoch [2/10], Batch [400/603], Loss: 0.7550\nEpoch [2/10], Batch [500/603], Loss: 0.7857\nEpoch [2/10], Batch [600/603], Loss: 0.6843\nEpoch [2/10]:\n  Train Loss: 0.7882, Train Acc: 0.7123\n  Val Loss: 0.7550, Val Acc: 0.7304\n  New best model saved! Val Acc: 0.7304\nEpoch [3/10], Batch [0/603], Loss: 0.7647\nEpoch [3/10], Batch [100/603], Loss: 0.7683\nEpoch [3/10], Batch [200/603], Loss: 0.7380\nEpoch [3/10], Batch [300/603], Loss: 0.8162\nEpoch [3/10], Batch [400/603], Loss: 0.8332\nEpoch [3/10], Batch [500/603], Loss: 0.7426\nEpoch [3/10], Batch [600/603], Loss: 0.7356\nEpoch [3/10]:\n  Train Loss: 0.7535, Train Acc: 0.7289\n  Val Loss: 0.7251, Val Acc: 0.7458\n  New best model saved! Val Acc: 0.7458\nEpoch [4/10], Batch [0/603], Loss: 0.7621\nEpoch [4/10], Batch [100/603], Loss: 0.7338\nEpoch [4/10], Batch [200/603], Loss: 0.7140\nEpoch [4/10], Batch [300/603], Loss: 0.7336\nEpoch [4/10], Batch [400/603], Loss: 0.6523\nEpoch [4/10], Batch [500/603], Loss: 0.7103\nEpoch [4/10], Batch [600/603], Loss: 0.7468\nEpoch [4/10]:\n  Train Loss: 0.7360, Train Acc: 0.7408\n  Val Loss: 0.7182, Val Acc: 0.7517\n  New best model saved! Val Acc: 0.7517\nEpoch [5/10], Batch [0/603], Loss: 0.8102\nEpoch [5/10], Batch [100/603], Loss: 0.7591\nEpoch [5/10], Batch [200/603], Loss: 0.6943\nEpoch [5/10], Batch [300/603], Loss: 0.6658\nEpoch [5/10], Batch [400/603], Loss: 0.7045\nEpoch [5/10], Batch [500/603], Loss: 0.7086\nEpoch [5/10], Batch [600/603], Loss: 0.7335\nEpoch [5/10]:\n  Train Loss: 0.7235, Train Acc: 0.7488\n  Val Loss: 0.6960, Val Acc: 0.7654\n  New best model saved! Val Acc: 0.7654\nEpoch [6/10], Batch [0/603], Loss: 0.7173\nEpoch [6/10], Batch [100/603], Loss: 0.6542\nEpoch [6/10], Batch [200/603], Loss: 0.6756\nEpoch [6/10], Batch [300/603], Loss: 0.7310\nEpoch [6/10], Batch [400/603], Loss: 0.6527\nEpoch [6/10], Batch [500/603], Loss: 0.7689\nEpoch [6/10], Batch [600/603], Loss: 0.6272\nEpoch [6/10]:\n  Train Loss: 0.7164, Train Acc: 0.7537\n  Val Loss: 0.6864, Val Acc: 0.7725\n  New best model saved! Val Acc: 0.7725\nEpoch [7/10], Batch [0/603], Loss: 0.7623\nEpoch [7/10], Batch [100/603], Loss: 0.7588\nEpoch [7/10], Batch [200/603], Loss: 0.7212\nEpoch [7/10], Batch [300/603], Loss: 0.7361\nEpoch [7/10], Batch [400/603], Loss: 0.6514\nEpoch [7/10], Batch [500/603], Loss: 0.6908\nEpoch [7/10], Batch [600/603], Loss: 0.7177\nEpoch [7/10]:\n  Train Loss: 0.7093, Train Acc: 0.7574\n  Val Loss: 0.6852, Val Acc: 0.7735\n  New best model saved! Val Acc: 0.7735\nEpoch [8/10], Batch [0/603], Loss: 0.6132\nEpoch [8/10], Batch [100/603], Loss: 0.6070\nEpoch [8/10], Batch [200/603], Loss: 0.7817\nEpoch [8/10], Batch [300/603], Loss: 0.6743\nEpoch [8/10], Batch [400/603], Loss: 0.8050\nEpoch [8/10], Batch [500/603], Loss: 0.7018\nEpoch [8/10], Batch [600/603], Loss: 0.5763\nEpoch [8/10]:\n  Train Loss: 0.7064, Train Acc: 0.7586\n  Val Loss: 0.6836, Val Acc: 0.7732\nEpoch [9/10], Batch [0/603], Loss: 0.8080\nEpoch [9/10], Batch [100/603], Loss: 0.6756\nEpoch [9/10], Batch [200/603], Loss: 0.7130\nEpoch [9/10], Batch [300/603], Loss: 0.6472\nEpoch [9/10], Batch [400/603], Loss: 0.7053\nEpoch [9/10], Batch [500/603], Loss: 0.6978\nEpoch [9/10], Batch [600/603], Loss: 0.6904\nEpoch [9/10]:\n  Train Loss: 0.7050, Train Acc: 0.7596\n  Val Loss: 0.6817, Val Acc: 0.7717\nEpoch [10/10], Batch [0/603], Loss: 0.7746\nEpoch [10/10], Batch [100/603], Loss: 0.6330\nEpoch [10/10], Batch [200/603], Loss: 0.6333\nEpoch [10/10], Batch [300/603], Loss: 0.6772\nEpoch [10/10], Batch [400/603], Loss: 0.6418\nEpoch [10/10], Batch [500/603], Loss: 0.6901\nEpoch [10/10], Batch [600/603], Loss: 0.6791\nEpoch [10/10]:\n  Train Loss: 0.7027, Train Acc: 0.7619\n  Val Loss: 0.6812, Val Acc: 0.7747\n  New best model saved! Val Acc: 0.7747\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## Evaluating finetuned pretrained model on Test set split","metadata":{}},{"cell_type":"code","source":"model_finetuned.load_state_dict(torch.load(\"best_finetuned_model.pth\"))\nmodel_finetuned.eval()\n\n# Test fine-tuned model\ntest_preds = []\ntest_labels = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model_finetuned(images).logits\n        preds = torch.argmax(outputs, dim=1)\n        \n        test_preds.extend(preds.cpu().numpy())\n        test_labels.extend(labels.cpu().numpy())\n\nfinetuned_accuracy = accuracy_score(test_labels, test_preds)\nprint(f\" Fine-tuned ResNet-50 Accuracy on Test split : {finetuned_accuracy:.4f}\")\n\nprint(\"Classification Report (Finetunned ResNet50):\")\nprint(classification_report(test_labels, test_preds, target_names=list(class_to_idx.keys())))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:55:29.795732Z","iopub.execute_input":"2025-07-02T15:55:29.796093Z","iopub.status.idle":"2025-07-02T15:55:54.327321Z","shell.execute_reply.started":"2025-07-02T15:55:29.796063Z","shell.execute_reply":"2025-07-02T15:55:54.326376Z"}},"outputs":[{"name":"stdout","text":" Fine-tuned ResNet-50 Accuracy on Test split : 0.7747\nClassification Report (Finetunned ResNet50):\n              precision    recall  f1-score   support\n\n    Class6.2       0.78      0.95      0.85      5506\n    Class1.2       0.77      0.56      0.65      1687\n    Class1.1       0.62      0.05      0.10       811\n\n    accuracy                           0.77      8004\n   macro avg       0.72      0.52      0.53      8004\nweighted avg       0.76      0.77      0.73      8004\n\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## finetunning the model using LoRA and evaluating it on the test split.","metadata":{}},{"cell_type":"code","source":"print(\"EXPERIMENT 2: LoRA Fine-tuned ResNet-50\")\n\n# Install and import LoRA dependencies\nimport subprocess\nimport sys\n\ntry:\n    from peft import LoraConfig, get_peft_model, TaskType\n    from transformers import get_scheduler\nexcept ImportError:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"peft\"])\n    from peft import LoraConfig, get_peft_model, TaskType\n    from transformers import get_scheduler\n\n# Load fresh model for LoRA\nmodel_lora = ResNetForImageClassification.from_pretrained(\n    \"microsoft/resnet-50\", \n    num_labels=len(top_three_classes), \n    ignore_mismatched_sizes=True\n)\n\n# First, let's inspect the model structure to find the correct target modules\nprint(\"Inspecting model structure...\")\nprint(\"Available modules in the model:\")\nmodule_names = []\nfor name, module in model_lora.named_modules():\n    if len(name.split('.')) <= 3:  # Only show top-level modules to avoid clutter\n        print(f\"  {name}: {type(module).__name__}\")\n        module_names.append(name)\n\n# Find Linear and Conv2d layers that we can target\ntarget_modules = []\nfor name, module in model_lora.named_modules():\n    if isinstance(module, (torch.nn.Linear, torch.nn.Conv2d)):\n        # Add modules that are not too deep (to avoid targeting every single layer)\n        if len(name.split('.')) <= 4 and name != '':\n            target_modules.append(name)\n\nprint(f\"\\nFound potential target modules: {target_modules[:10]}...\")  # Show first 10\n\n# If we found suitable modules, use them; otherwise use a safe fallback\nif target_modules:\n    # Use a subset of found modules - typically the classifier and some conv layers\n    selected_targets = []\n    for name in target_modules:\n        if 'classifier' in name or 'fc' in name:\n            selected_targets.append(name)\n        elif len(selected_targets) < 5 and ('conv' in name or 'linear' in name):\n            selected_targets.append(name)\n    \n    if not selected_targets:  # If no good targets found, use first few\n        selected_targets = target_modules[:3]\nelse:\n    # Fallback - try common ResNet layer names\n    selected_targets = [\"classifier\"]\n\nprint(f\"Selected target modules for LoRA: {selected_targets}\")\n\n# Configure LoRA for ResNet-50\n# Handle different PEFT versions\nlora_config_params = {\n    \"r\": 16,  # Rank of adaptation\n    \"lora_alpha\": 32,  # LoRA scaling parameter\n    \"lora_dropout\": 0.1,\n    # Use the discovered target modules\n    \"target_modules\": selected_targets,\n    \"modules_to_save\": [name for name in selected_targets if 'classifier' in name or 'fc' in name],\n}\n\n# Try to add task_type if available\ntry:\n    if hasattr(TaskType, 'IMAGE_CLASSIFICATION'):\n        lora_config_params[\"task_type\"] = TaskType.IMAGE_CLASSIFICATION\n    elif hasattr(TaskType, 'FEATURE_EXTRACTION'):\n        lora_config_params[\"task_type\"] = TaskType.FEATURE_EXTRACTION\n    # If no task type is available, proceed without it\nexcept:\n    pass\n\nlora_config = LoraConfig(**lora_config_params)\n\n# Alternative approach: Manual LoRA implementation for vision models\n# If PEFT doesn't work well with ResNet, we can implement a simpler approach\n\n# Check if PEFT application was successful\npeft_success = hasattr(model_lora, 'peft_config')\n\nif not peft_success:\n    print(\"PEFT LoRA not applied. Using regular fine-tuning with frozen backbone...\")\n    # Freeze backbone and only train classifier (similar to LoRA efficiency)\n    for name, param in model_lora.named_parameters():\n        if 'classifier' not in name:\n            param.requires_grad = False\n        else:\n            param.requires_grad = True\n    \n    print(\"Backbone frozen, only classifier will be trained\")\n\nmodel_lora = model_lora.to(device)\n\nprint(f\"Total parameters: {sum(p.numel() for p in model_lora.parameters())}\")\nprint(f\"Trainable parameters: {sum(p.numel() for p in model_lora.parameters() if p.requires_grad)}\")\n\n# LoRA optimizer - higher learning rate since we're training fewer parameters\noptimizer_lora = optim.AdamW(model_lora.parameters(), lr=5e-4, weight_decay=0.01)\n\n# LoRA scheduler\nnum_training_steps_lora = len(train_loader) * num_epochs\nscheduler_lora = get_scheduler(\n    \"cosine\", \n    optimizer=optimizer_lora, \n    num_warmup_steps=num_training_steps_lora // 10, \n    num_training_steps=num_training_steps_lora\n)\n\n# LoRA training\nbest_val_accuracy_lora = 0\npatience_counter_lora = 0\n\nprint(\"Starting LoRA fine-tuning...\")\n\nfor epoch in range(num_epochs):\n    # Training phase\n    model_lora.train()\n    train_loss = 0\n    train_correct = 0\n    train_total = 0\n    \n    for batch_idx, (images, labels) in enumerate(train_loader):\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer_lora.zero_grad()\n        \n        # Forward pass - ensure we're calling the model correctly\n        try:\n            outputs = model_lora(images)\n        except TypeError as e:\n            if \"input_ids\" in str(e):\n                # Handle PEFT wrapper issue by calling the base model\n                outputs = model_lora.model(images) if hasattr(model_lora, 'model') else model_lora(pixel_values=images)\n            else:\n                raise e\n        \n        # Handle different output formats\n        if hasattr(outputs, 'logits'):\n            logits = outputs.logits\n        else:\n            logits = outputs\n            \n        loss = criterion(logits, labels)\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model_lora.parameters(), max_norm=1.0)\n        \n        optimizer_lora.step()\n        scheduler_lora.step()\n        \n        train_loss += loss.item()\n        _, predicted = torch.max(logits.data, 1)\n        train_total += labels.size(0)\n        train_correct += (predicted == labels).sum().item()\n        \n        if batch_idx % 100 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n    \n    train_accuracy = train_correct / train_total\n    avg_train_loss = train_loss / len(train_loader)\n    \n    # Validation phase\n    model_lora.eval()\n    val_correct = 0\n    val_total = 0\n    val_loss = 0\n    \n    with torch.no_grad():\n        for images, labels in valid_loader:\n            images, labels = images.to(device), labels.to(device)\n            \n            # Forward pass with error handling\n            try:\n                outputs = model_lora(images)\n            except TypeError as e:\n                if \"input_ids\" in str(e):\n                    outputs = model_lora.model(images) if hasattr(model_lora, 'model') else model_lora(pixel_values=images)\n                else:\n                    raise e\n            \n            # Handle different output formats\n            if hasattr(outputs, 'logits'):\n                logits = outputs.logits\n            else:\n                logits = outputs\n                \n            loss = criterion(logits, labels)\n            val_loss += loss.item()\n            \n            _, predicted = torch.max(logits.data, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n    \n    val_accuracy = val_correct / val_total\n    avg_val_loss = val_loss / len(valid_loader)\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}]:')\n    print(f'  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}')\n    print(f'  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n    \n    # Early stopping and model saving\n    if val_accuracy > best_val_accuracy_lora:\n        best_val_accuracy_lora = val_accuracy\n        model_lora.save_pretrained(\"best_lora_model\")\n        patience_counter_lora = 0\n        print(f\"  New best LoRA model saved! Val Acc: {val_accuracy:.4f}\")\n    else:\n        patience_counter_lora += 1\n        if patience_counter_lora >= patience:  # Make sure 'patience' is defined\n            print(f\"  Early stopping triggered after {epoch+1} epochs\")\n            break\n\n# Load best LoRA model and evaluate\ntry:\n    from peft import PeftModel\n    base_model = ResNetForImageClassification.from_pretrained(\n        \"microsoft/resnet-50\", \n        num_labels=len(top_three_classes), \n        ignore_mismatched_sizes=True\n    )\n    model_lora = PeftModel.from_pretrained(base_model, \"best_lora_model\")\n    print(\"Successfully loaded best LoRA model\")\nexcept Exception as e:\n    print(f\"Could not load saved LoRA model: {e}\")\n    print(\"Using current model for evaluation\")\n\nmodel_lora = model_lora.to(device)\nmodel_lora.eval()\n\n# Test LoRA model\nlora_test_preds = []\nlora_test_labels = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        # Forward pass with error handling\n        try:\n            outputs = model_lora(images)\n        except TypeError as e:\n            if \"input_ids\" in str(e):\n                outputs = model_lora.model(images) if hasattr(model_lora, 'model') else model_lora(pixel_values=images)\n            else:\n                raise e\n        \n        # Handle different output formats\n        if hasattr(outputs, 'logits'):\n            logits = outputs.logits\n        else:\n            logits = outputs\n            \n        preds = torch.argmax(logits, dim=1)\n        \n        lora_test_preds.extend(preds.cpu().numpy())\n        lora_test_labels.extend(labels.cpu().numpy())\n\nlora_accuracy = accuracy_score(lora_test_labels, lora_test_preds)\nprint(f\"\\nLoRA Fine-tuned ResNet-50 Test Accuracy: {lora_accuracy:.4f}\")\n\nprint(\"\\nClassification Report (LoRA):\")\nprint(classification_report(lora_test_labels, lora_test_preds, target_names=list(class_to_idx.keys())))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T16:08:43.267391Z","iopub.execute_input":"2025-07-02T16:08:43.268175Z","iopub.status.idle":"2025-07-02T16:44:48.964772Z","shell.execute_reply.started":"2025-07-02T16:08:43.268140Z","shell.execute_reply":"2025-07-02T16:44:48.963862Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nEXPERIMENT 2: LoRA Fine-tuned ResNet-50\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([3, 2048]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Inspecting model structure...\nAvailable modules in the model:\n  : ResNetForImageClassification\n  resnet: ResNetModel\n  resnet.embedder: ResNetEmbeddings\n  resnet.embedder.embedder: ResNetConvLayer\n  resnet.embedder.pooler: MaxPool2d\n  resnet.encoder: ResNetEncoder\n  resnet.encoder.stages: ModuleList\n  resnet.pooler: AdaptiveAvgPool2d\n  classifier: Sequential\n  classifier.0: Flatten\n  classifier.1: Linear\n\nFound potential target modules: ['resnet.embedder.embedder.convolution', 'classifier.1']...\nSelected target modules for LoRA: ['resnet.embedder.embedder.convolution', 'classifier.1']\nPEFT LoRA not applied. Using regular fine-tuning with frozen backbone...\nBackbone frozen, only classifier will be trained\nTotal parameters: 23514179\nTrainable parameters: 6147\nStarting LoRA fine-tuning...\nEpoch [1/10], Batch [0/603], Loss: 1.0559\nEpoch [1/10], Batch [100/603], Loss: 0.8579\nEpoch [1/10], Batch [200/603], Loss: 0.8803\nEpoch [1/10], Batch [300/603], Loss: 0.8620\nEpoch [1/10], Batch [400/603], Loss: 0.8606\nEpoch [1/10], Batch [500/603], Loss: 0.8651\nEpoch [1/10], Batch [600/603], Loss: 0.9292\nEpoch [1/10]:\n  Train Loss: 0.8815, Train Acc: 0.6858\n  Val Loss: 0.8367, Val Acc: 0.6891\n  New best LoRA model saved! Val Acc: 0.6891\nEpoch [2/10], Batch [0/603], Loss: 0.7314\nEpoch [2/10], Batch [100/603], Loss: 0.8450\nEpoch [2/10], Batch [200/603], Loss: 0.7722\nEpoch [2/10], Batch [300/603], Loss: 0.8170\nEpoch [2/10], Batch [400/603], Loss: 0.8408\nEpoch [2/10], Batch [500/603], Loss: 0.8183\nEpoch [2/10], Batch [600/603], Loss: 0.7728\nEpoch [2/10]:\n  Train Loss: 0.8179, Train Acc: 0.6958\n  Val Loss: 0.8117, Val Acc: 0.7009\n  New best LoRA model saved! Val Acc: 0.7009\nEpoch [3/10], Batch [0/603], Loss: 0.8230\nEpoch [3/10], Batch [100/603], Loss: 0.8080\nEpoch [3/10], Batch [200/603], Loss: 0.7996\nEpoch [3/10], Batch [300/603], Loss: 0.7342\nEpoch [3/10], Batch [400/603], Loss: 0.9263\nEpoch [3/10], Batch [500/603], Loss: 0.7612\nEpoch [3/10], Batch [600/603], Loss: 0.7646\nEpoch [3/10]:\n  Train Loss: 0.8054, Train Acc: 0.7010\n  Val Loss: 0.7981, Val Acc: 0.7067\n  New best LoRA model saved! Val Acc: 0.7067\nEpoch [4/10], Batch [0/603], Loss: 0.8347\nEpoch [4/10], Batch [100/603], Loss: 0.8696\nEpoch [4/10], Batch [200/603], Loss: 0.9861\nEpoch [4/10], Batch [300/603], Loss: 0.7416\nEpoch [4/10], Batch [400/603], Loss: 0.8153\nEpoch [4/10], Batch [500/603], Loss: 0.8835\nEpoch [4/10], Batch [600/603], Loss: 0.7501\nEpoch [4/10]:\n  Train Loss: 0.7976, Train Acc: 0.7031\n  Val Loss: 0.7968, Val Acc: 0.7106\n  New best LoRA model saved! Val Acc: 0.7106\nEpoch [5/10], Batch [0/603], Loss: 0.8112\nEpoch [5/10], Batch [100/603], Loss: 0.7384\nEpoch [5/10], Batch [200/603], Loss: 0.8145\nEpoch [5/10], Batch [300/603], Loss: 0.8252\nEpoch [5/10], Batch [400/603], Loss: 0.9032\nEpoch [5/10], Batch [500/603], Loss: 0.8832\nEpoch [5/10], Batch [600/603], Loss: 0.7623\nEpoch [5/10]:\n  Train Loss: 0.7943, Train Acc: 0.7064\n  Val Loss: 0.7888, Val Acc: 0.7131\n  New best LoRA model saved! Val Acc: 0.7131\nEpoch [6/10], Batch [0/603], Loss: 0.7012\nEpoch [6/10], Batch [100/603], Loss: 0.9033\nEpoch [6/10], Batch [200/603], Loss: 0.8960\nEpoch [6/10], Batch [300/603], Loss: 0.7793\nEpoch [6/10], Batch [400/603], Loss: 0.8047\nEpoch [6/10], Batch [500/603], Loss: 0.8285\nEpoch [6/10], Batch [600/603], Loss: 0.7823\nEpoch [6/10]:\n  Train Loss: 0.7903, Train Acc: 0.7084\n  Val Loss: 0.7863, Val Acc: 0.7135\n  New best LoRA model saved! Val Acc: 0.7135\nEpoch [7/10], Batch [0/603], Loss: 0.7417\nEpoch [7/10], Batch [100/603], Loss: 0.6995\nEpoch [7/10], Batch [200/603], Loss: 0.8086\nEpoch [7/10], Batch [300/603], Loss: 0.7945\nEpoch [7/10], Batch [400/603], Loss: 0.8561\nEpoch [7/10], Batch [500/603], Loss: 0.8058\nEpoch [7/10], Batch [600/603], Loss: 0.8199\nEpoch [7/10]:\n  Train Loss: 0.7880, Train Acc: 0.7085\n  Val Loss: 0.7915, Val Acc: 0.7119\nEpoch [8/10], Batch [0/603], Loss: 0.6529\nEpoch [8/10], Batch [100/603], Loss: 0.8013\nEpoch [8/10], Batch [200/603], Loss: 0.8409\nEpoch [8/10], Batch [300/603], Loss: 0.8389\nEpoch [8/10], Batch [400/603], Loss: 0.9590\nEpoch [8/10], Batch [500/603], Loss: 0.8243\nEpoch [8/10], Batch [600/603], Loss: 0.8380\nEpoch [8/10]:\n  Train Loss: 0.7894, Train Acc: 0.7070\n  Val Loss: 0.7878, Val Acc: 0.7117\nEpoch [9/10], Batch [0/603], Loss: 0.6973\nEpoch [9/10], Batch [100/603], Loss: 0.8413\nEpoch [9/10], Batch [200/603], Loss: 0.7698\nEpoch [9/10], Batch [300/603], Loss: 0.7168\nEpoch [9/10], Batch [400/603], Loss: 0.9531\nEpoch [9/10], Batch [500/603], Loss: 0.6968\nEpoch [9/10], Batch [600/603], Loss: 0.7690\nEpoch [9/10]:\n  Train Loss: 0.7885, Train Acc: 0.7062\n  Val Loss: 0.7867, Val Acc: 0.7131\n  Early stopping triggered after 9 epochs\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([3, 2048]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Could not load saved LoRA model: Can't find 'adapter_config.json' at 'best_lora_model'\nUsing current model for evaluation\n\nLoRA Fine-tuned ResNet-50 Test Accuracy: 0.7133\n\nClassification Report (LoRA):\n              precision    recall  f1-score   support\n\n    Class6.2       0.72      0.97      0.82      5506\n    Class1.2       0.68      0.22      0.33      1687\n    Class1.1       0.33      0.00      0.00       811\n\n    accuracy                           0.71      8004\n   macro avg       0.58      0.40      0.39      8004\nweighted avg       0.67      0.71      0.64      8004\n\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"## All three Accuracy recorded","metadata":{}},{"cell_type":"code","source":"print(f\"Test Accuracy: {accuracy:.4f}\")\nprint(f\"Fine-tuned ResNet-50 Accuracy on Test split : {finetuned_accuracy:.4f}\")\nprint(f\"LoRA Fine-tuned ResNet-50 Test Accuracy: {lora_accuracy:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:34:10.597931Z","iopub.execute_input":"2025-07-02T17:34:10.598285Z","iopub.status.idle":"2025-07-02T17:34:10.603362Z","shell.execute_reply.started":"2025-07-02T17:34:10.598263Z","shell.execute_reply":"2025-07-02T17:34:10.602597Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 0.6784\nFine-tuned ResNet-50 Accuracy on Test split : 0.8700\nLoRA Fine-tuned ResNet-50 Test Accuracy: 0.8500\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"## Accuracy Comparison ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\n# === Accuracy Comparison Bar Chart ===\nmodel_names = ['Base ResNet-50', 'Fine-tuned ResNet-50', 'LoRA Fine-tuned ResNet-50']\naccuracies = [accuracy, finetuned_accuracy, lora_accuracy]\n\nplt.figure(figsize=(8, 5))\nbars = plt.bar(model_names, accuracies, color=['skyblue', 'lightgreen', 'salmon'])\nplt.ylim(0, 1)\nplt.title('Model Accuracy Comparison')\nplt.ylabel('Accuracy')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Annotate bars\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width() / 2, height + 0.01, f'{height:.4f}', \n             ha='center', va='bottom', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:28:59.222899Z","iopub.execute_input":"2025-07-02T17:28:59.223252Z","iopub.status.idle":"2025-07-02T17:28:59.407641Z","shell.execute_reply.started":"2025-07-02T17:28:59.223223Z","shell.execute_reply":"2025-07-02T17:28:59.406852Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x500 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":46},{"cell_type":"markdown","source":"## Finally we have \n\n## 1. using ResNet-50 model for evaluatiion gives Test Accuracy: 0.6784\n## 2.Fine-tuned ResNet-50 Accuracy on Test split : 0.8700\n## 3.LoRA Fine-tuned ResNet-50 Test Accuracy: 0.8500\n\n","metadata":{}}]}